{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from nlp_fast_unlearning.utils import prepare_dbpedia, build_noisy_dl, ensure_deterministic, DEVICE, BATCH_SIZE\n",
        "from nlp_fast_unlearning.baseline_model import TextClassificationModel\n",
        "\n",
        "_, _, _, dbpedia_vocab = prepare_dbpedia(for_baseline_only=True)\n",
        "\n",
        "vocab_size = dbpedia_vocab.vocab_size\n",
        "\n",
        "baseline_name = \"DBpedia_baseline.pt\"\n",
        "unlearning_model = TextClassificationModel(vocab_size).to(DEVICE)\n",
        "unlearning_model.load_state_dict(torch.load(baseline_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Searching for error maximizing noise for class  1\n",
            "Got loss 222.51974487304688 for tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
            "        101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
            "        101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
            "        101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
            "        101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
            "        101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
            "        101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
            "        101, 101], device='cuda:0')\n",
            "Searching for error maximizing noise for class  3\n",
            "Got loss 201.43650817871094 for tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
            "        101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
            "        101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
            "        101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
            "        101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
            "        101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
            "        101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
            "        101, 101], device='cuda:0')\n",
            "Class 1 (Company) original samples: 37965\n",
            "Class 2 (EducationalInstitution) original samples: 38032\n",
            "Class 3 (Artist) original samples: 38046\n",
            "Class 4 (Athlete) original samples: 38007\n",
            "Class 5 (OfficeHolder) original samples: 38000\n",
            "Class 6 (MeanOfTransportation) original samples: 38070\n",
            "Class 7 (Building) original samples: 38035\n",
            "Class 8 (NaturalPlace) original samples: 37970\n",
            "Class 9 (Village) original samples: 37992\n",
            "Class 10 (Animal) original samples: 38016\n",
            "Class 11 (Plant) original samples: 37931\n",
            "Class 12 (Album) original samples: 37917\n",
            "Class 13 (Film) original samples: 37994\n",
            "Class 14 (WrittenWork) original samples: 38025\n",
            "CPU times: user 37.2 s, sys: 440 ms, total: 37.6 s\n",
            "Wall time: 37.3 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "(\n",
        "    retain_samples,\n",
        "    noisy_data,\n",
        "    retain_valid_dl,\n",
        "    forget_valid_dl,\n",
        "    retain_test_dl,\n",
        "    forget_test_dl,\n",
        "    dbpedia_vocab,\n",
        ") = prepare_dbpedia(\n",
        "    for_baseline_only=False,\n",
        "    classes_to_forget=[1, 3],\n",
        "    model=unlearning_model,\n",
        "    retain_percentage=0.01,\n",
        "    vocab_class=dbpedia_vocab,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
        "\n",
        "\n",
        "def validation_step(model, batch):\n",
        "    labels, text, offsets = batch\n",
        "    out = model(text,offsets)\n",
        "    loss = F.cross_entropy(out, labels)   \n",
        "    acc = accuracy(out, labels)\n",
        "    return {'Loss': loss.detach(), 'Acc': acc}\n",
        "\n",
        "\n",
        "def validation_epoch_end(model, outputs):\n",
        "    batch_losses = [x['Loss'] for x in outputs]\n",
        "    epoch_loss = torch.stack(batch_losses).mean()   \n",
        "    batch_accs = [x['Acc'] for x in outputs]\n",
        "    epoch_acc = torch.stack(batch_accs).mean()      \n",
        "    return {'Loss': epoch_loss.item(), 'Acc': epoch_acc.item()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_after_unlearning(model, val_loader):\n",
        "    model.eval()\n",
        "    outputs = [validation_step(model, batch) for batch in val_loader]\n",
        "    return validation_epoch_end(model, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|                                                            | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss 1: 7057.12420241038,Train Acc:45.16511041439477%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.06s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best hyperparams: LR=12, grad clip=22, ratio=1.5 | Best retain acc: 97.17997312545776\n",
            "CPU times: user 2.52 s, sys: 384 ms, total: 2.91 s\n",
            "Wall time: 3.58 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "lrs = [12]\n",
        "clips = [22]\n",
        "ratios = [1.5]\n",
        "\n",
        "# The following search candidates take a few minutes to run through\n",
        "# lrs = [7,8,9,10,11,12,13,14]\n",
        "# clips = [17,18,19,20,21,22,23,24]\n",
        "# ratios = [0.8,1,1.5]\n",
        "\n",
        "\n",
        "best_retain_acc = 0\n",
        "\n",
        "unlearned_model_name = \"DBpedia_fast_unlearned.pt\"\n",
        "\n",
        "for lr in tqdm(lrs):\n",
        "    for clip in clips:\n",
        "        for ratio in ratios:\n",
        "            ensure_deterministic()\n",
        "            \n",
        "            noisy_loader = build_noisy_dl(\n",
        "                retain_samples,\n",
        "                noisy_data,\n",
        "                dbpedia_vocab,\n",
        "                retain_to_forget_ratio=ratio,\n",
        "            )\n",
        "            unlearning_model = TextClassificationModel(vocab_size).to(DEVICE)\n",
        "            unlearning_model.load_state_dict(torch.load(baseline_name))\n",
        "\n",
        "            optimizer = torch.optim.SGD(unlearning_model.parameters(), lr = lr)\n",
        "\n",
        "\n",
        "            unlearning_model.train(True)\n",
        "            for epoch in range(1):\n",
        "                running_loss = 0.0\n",
        "                running_acc = 0\n",
        "                num_batches = len(noisy_loader)\n",
        "                \n",
        "                for i, data in enumerate(noisy_loader):\n",
        "                    labels, inputs, offsets = data\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = unlearning_model(inputs,offsets)\n",
        "                    loss = unlearning_model.criterion(outputs, labels)\n",
        "                    loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(unlearning_model.parameters(), clip)\n",
        "                    optimizer.step()\n",
        "\n",
        "                    running_loss += loss.item()\n",
        "                    out = torch.argmax(outputs.detach(),dim=1)\n",
        "                    running_acc += (labels==out).sum().item()/labels.size(0)\n",
        "                print(f\"Train loss {epoch+1}: {running_loss/num_batches},Train Acc:{running_acc*100/num_batches}%\")\n",
        "                forget_acc = evaluate_after_unlearning(unlearning_model, forget_valid_dl)[\"Acc\"]*100\n",
        "                if forget_acc == 0.0:\n",
        "                    retain_acc = evaluate_after_unlearning(unlearning_model, retain_valid_dl)[\"Acc\"]*100\n",
        "                    if retain_acc > best_retain_acc:\n",
        "                        best_retain_acc = retain_acc\n",
        "                        best_lr = lr\n",
        "                        best_clip = clip\n",
        "                        best_ratio = ratio\n",
        "                        torch.save(unlearning_model.state_dict(), unlearned_model_name)\n",
        "\n",
        "print(\n",
        "    f\"Best hyperparams: LR={best_lr}, grad clip={best_clip}, \"\n",
        "    f\"ratio={best_ratio} | Best retain acc: {best_retain_acc}\"\n",
        ")\n",
        "unlearning_model = TextClassificationModel(vocab_size).to(DEVICE)\n",
        "unlearning_model.load_state_dict(torch.load(unlearned_model_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performance of Standard Forget Model on Forget Class\n",
            "Accuracy: 0.0\n",
            "Loss: 65.2414321899414\n",
            "Performance of Standard Forget Model on Retain Class\n",
            "Accuracy: 97.18105792999268\n",
            "Loss: 0.4106481671333313\n"
          ]
        }
      ],
      "source": [
        "print(\"Performance of Standard Forget Model on Forget Class\")\n",
        "history = [evaluate_after_unlearning(unlearning_model, forget_valid_dl)]\n",
        "print(\"Accuracy: {}\".format(history[0][\"Acc\"]*100))\n",
        "print(\"Loss: {}\".format(history[0][\"Loss\"]))\n",
        "\n",
        "print(\"Performance of Standard Forget Model on Retain Class\")\n",
        "history = [evaluate_after_unlearning(unlearning_model, retain_valid_dl)]\n",
        "print(\"Accuracy: {}\".format(history[0][\"Acc\"]*100))\n",
        "print(\"Loss: {}\".format(history[0][\"Loss\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test on Forget Class\n",
            "Accuracy: 0.0\n",
            "Loss: 64.73976135253906\n",
            "Test on Retain Class\n",
            "Accuracy: 96.82180881500244\n",
            "Loss: 0.28862181305885315\n"
          ]
        }
      ],
      "source": [
        "print(\"Test on Forget Class\")\n",
        "history = [evaluate_after_unlearning(unlearning_model, forget_test_dl)]\n",
        "print(\"Accuracy: {}\".format(history[0][\"Acc\"]*100))\n",
        "print(\"Loss: {}\".format(history[0][\"Loss\"]))\n",
        "\n",
        "print(\"Test on Retain Class\")\n",
        "history = [evaluate_after_unlearning(unlearning_model, retain_test_dl)]\n",
        "print(\"Accuracy: {}\".format(history[0][\"Acc\"]*100))\n",
        "print(\"Loss: {}\".format(history[0][\"Loss\"]))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
