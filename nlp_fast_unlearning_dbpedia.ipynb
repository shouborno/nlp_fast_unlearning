{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Searching for error maximizing noise for class  1\n",
            "Got loss 222.51974487304688 for tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
            "        101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
            "        101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
            "        101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
            "        101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
            "        101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
            "        101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
            "        101, 101], device='cuda:0')\n",
            "Searching for error maximizing noise for class  3\n",
            "Got loss 226.35386657714844 for tensor([36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36,\n",
            "        36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36,\n",
            "        36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36,\n",
            "        36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36,\n",
            "        36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36,\n",
            "        36, 36, 36, 36, 36, 36, 36, 36, 36, 36], device='cuda:0')\n",
            "Class 1 (Company) original samples: 37965\n",
            "Class 2 (EducationalInstitution) original samples: 38032\n",
            "Class 3 (Artist) original samples: 38046\n",
            "Class 4 (Athlete) original samples: 38007\n",
            "Class 5 (OfficeHolder) original samples: 38000\n",
            "Class 6 (MeanOfTransportation) original samples: 38070\n",
            "Class 7 (Building) original samples: 38035\n",
            "Class 8 (NaturalPlace) original samples: 37970\n",
            "Class 9 (Village) original samples: 37992\n",
            "Class 10 (Animal) original samples: 38016\n",
            "Class 11 (Plant) original samples: 37931\n",
            "Class 12 (Album) original samples: 37917\n",
            "Class 13 (Film) original samples: 37994\n",
            "Class 14 (WrittenWork) original samples: 38025\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from nlp_fast_unlearning.utils import prepare_dbpedia, build_noisy_dl, ensure_deterministic, DEVICE, BATCH_SIZE\n",
        "from nlp_fast_unlearning.baseline_model import TextClassificationModel\n",
        "\n",
        "_, _, _, vocab_size = prepare_dbpedia(for_baseline_only=True)\n",
        "\n",
        "baseline_name = \"DBpedia_baseline.pt\"\n",
        "unlearning_model = TextClassificationModel(vocab_size).to(DEVICE)\n",
        "unlearning_model.load_state_dict(torch.load(baseline_name))\n",
        "\n",
        "(\n",
        "    retain_samples,\n",
        "    noisy_data,\n",
        "    retain_valid_dl,\n",
        "    forget_valid_dl,\n",
        "    retain_test_dl,\n",
        "    forget_test_dl,\n",
        "    dbpedia_vocab,\n",
        ") = prepare_dbpedia(\n",
        "    for_baseline_only=False,\n",
        "    classes_to_forget=[1, 3],\n",
        "    model=unlearning_model,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
        "\n",
        "\n",
        "def validation_step(model, batch):\n",
        "    labels, text, offsets = batch\n",
        "    out = model(text,offsets)\n",
        "    loss = F.cross_entropy(out, labels)   \n",
        "    acc = accuracy(out, labels)\n",
        "    return {'Loss': loss.detach(), 'Acc': acc}\n",
        "\n",
        "\n",
        "def validation_epoch_end(model, outputs):\n",
        "    batch_losses = [x['Loss'] for x in outputs]\n",
        "    epoch_loss = torch.stack(batch_losses).mean()   \n",
        "    batch_accs = [x['Acc'] for x in outputs]\n",
        "    epoch_acc = torch.stack(batch_accs).mean()      \n",
        "    return {'Loss': epoch_loss.item(), 'Acc': epoch_acc.item()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_after_unlearning(model, val_loader):\n",
        "    model.eval()\n",
        "    outputs = [validation_step(model, batch) for batch in val_loader]\n",
        "    return validation_epoch_end(model, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|                                                                             | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:15<00:00, 15.85s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best hyperparams: LR=13, grad clip=15, ratio=0.1 | Best retain acc: 97.57819771766663\n",
            "CPU times: user 15.7 s, sys: 404 ms, total: 16.1 s\n",
            "Wall time: 16.4 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "from tqdm import tqdm\n",
        "\n",
        "lrs = [13]\n",
        "clips = [15]\n",
        "ratios = [0.1]\n",
        "\n",
        "best_retain_acc = 0\n",
        "\n",
        "unlearned_model_name = \"DBpedia_fast_unlearned.pt\"\n",
        "\n",
        "for lr in tqdm(lrs):\n",
        "    for clip in clips:\n",
        "        for ratio in ratios:\n",
        "            ensure_deterministic()\n",
        "            \n",
        "            noisy_loader = build_noisy_dl(\n",
        "                retain_samples,\n",
        "                noisy_data,\n",
        "                dbpedia_vocab,\n",
        "                retain_to_forget_ratio=ratio,\n",
        "            )\n",
        "            unlearning_model = TextClassificationModel(vocab_size).to(DEVICE)\n",
        "            unlearning_model.load_state_dict(torch.load(baseline_name))\n",
        "\n",
        "            optimizer = torch.optim.SGD(unlearning_model.parameters(), lr = lr)\n",
        "\n",
        "\n",
        "            unlearning_model.train(True)\n",
        "            for epoch in range(1):\n",
        "                running_loss = 0.0\n",
        "                running_acc = 0\n",
        "                num_batches = len(noisy_loader)\n",
        "                \n",
        "                for i, data in enumerate(noisy_loader):\n",
        "                    labels, inputs, offsets = data\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = unlearning_model(inputs,offsets)\n",
        "                    loss = unlearning_model.criterion(outputs, labels)\n",
        "                    loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(unlearning_model.parameters(), clip)\n",
        "                    optimizer.step()\n",
        "\n",
        "                    running_loss += loss.item()\n",
        "                    out = torch.argmax(outputs.detach(),dim=1)\n",
        "                    running_acc += (labels==out).sum().item()/labels.size(0)\n",
        "                # print(f\"Train loss {epoch+1}: {running_loss/num_batches},Train Acc:{running_acc*100/num_batches}%\")\n",
        "                forget_acc = evaluate_after_unlearning(unlearning_model, forget_valid_dl)[\"Acc\"]*100\n",
        "                if forget_acc == 0.0:\n",
        "                    retain_acc = evaluate_after_unlearning(unlearning_model, retain_valid_dl)[\"Acc\"]*100\n",
        "                    if retain_acc > best_retain_acc:\n",
        "                        best_retain_acc = retain_acc\n",
        "                        best_lr = lr\n",
        "                        best_clip = clip\n",
        "                        best_ratio = ratio\n",
        "                        torch.save(unlearning_model.state_dict(), unlearned_model_name)\n",
        "\n",
        "print(\n",
        "    f\"Best hyperparams: LR={best_lr}, grad clip={best_clip}, \"\n",
        "    f\"ratio={best_ratio} | Best retain acc: {best_retain_acc}\"\n",
        ")\n",
        "unlearning_model = TextClassificationModel(vocab_size).to(DEVICE)\n",
        "unlearning_model.load_state_dict(torch.load(unlearned_model_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performance of Standard Forget Model on Forget Class\n",
            "Accuracy: 0.0\n",
            "Loss: 365.3481750488281\n",
            "Performance of Standard Forget Model on Retain Class\n",
            "Accuracy: 97.5760281085968\n",
            "Loss: 0.18762344121932983\n"
          ]
        }
      ],
      "source": [
        "print(\"Performance of Standard Forget Model on Forget Class\")\n",
        "history = [evaluate_after_unlearning(unlearning_model, forget_valid_dl)]\n",
        "print(\"Accuracy: {}\".format(history[0][\"Acc\"]*100))\n",
        "print(\"Loss: {}\".format(history[0][\"Loss\"]))\n",
        "\n",
        "print(\"Performance of Standard Forget Model on Retain Class\")\n",
        "history = [evaluate_after_unlearning(unlearning_model, retain_valid_dl)]\n",
        "print(\"Accuracy: {}\".format(history[0][\"Acc\"]*100))\n",
        "print(\"Loss: {}\".format(history[0][\"Loss\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test on Forget Class\n",
            "Accuracy: 0.0\n",
            "Loss: 366.2167663574219\n",
            "Test on Retain Class\n",
            "Accuracy: 97.68394827842712\n",
            "Loss: 0.14733879268169403\n"
          ]
        }
      ],
      "source": [
        "print(\"Test on Forget Class\")\n",
        "history = [evaluate_after_unlearning(unlearning_model, forget_test_dl)]\n",
        "print(\"Accuracy: {}\".format(history[0][\"Acc\"]*100))\n",
        "print(\"Loss: {}\".format(history[0][\"Loss\"]))\n",
        "\n",
        "print(\"Test on Retain Class\")\n",
        "history = [evaluate_after_unlearning(unlearning_model, retain_test_dl)]\n",
        "print(\"Accuracy: {}\".format(history[0][\"Acc\"]*100))\n",
        "print(\"Loss: {}\".format(history[0][\"Loss\"]))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
